Анализ задачи

a.	Что было сделано.

Был создан парсер, собирающий данные с сайта https://quotes.toscrape.com/. Проект для удобства разбит на два файла, а в папке result сохранён итоговый json-файл со всей собранной информацией.
Были получены следующие сведения: текст цитаты, данные об авторе (имя и фамилия, дата рождения в формате даты, место рождения, биография), а также теги цитаты.
В файле parser.py представлена функция для сбора данных со всех страниц указанного ресурса. Перед сбором информации производится авторизация, в ходе которой в форму для входа пользователя передаются логин, пароль и предварительно полученный csrf_token. Непосредственно парсинг осуществляет функция-генератор get_quotes(), которая порционно выдаёт данные для последующей их записи в json-файл.
Запись сведений реализована в файле to_json.py. Данные в виде словарей сохраняются в список, который затем записывается в json-файл. Для парсинга и сохранения в json-файл информации с указанного сайта необходимо запустить код в файле to_json.py.

b.	Откуда были получены данные.

Данные были получены с сайта https://quotes.toscrape.com/, на котором представлены цитаты известных личностей. Помимо высказываний на ресурсе содержится также краткая информация о каждом из их авторов.

c.	Как осуществлялся сбор.

Сбор данных осуществлялся с помощью библиотеки BeautifulSoup в связке с requests и lxml.  
BeautifulSoup  производит поиск нужных элементов в полученном коде. Библиотека requests позволяет отправлять https запросы и получать ответ – код конкретной страницы. lxml анализирует полученный библиотекой requests код и подготавливает его в удобный вид для библиотеки BeautifulSoup.
В начале были объявлены данные для авторизации, заголовки для корректной обработки запросов. Ссылка на главную страницу была сохранена в переменную.
В ходе работы использовался класс Session из библиотеки requests. Это позволило помимо получения html-кода запрашиваемых страниц, сохранять ещё и файлы cookie. 
Для получения необходимой информации использовались следующие методы: метод find(), позволяющий найти в коде интересующий элемент по тегу и его классу, а также метод get(), дающий возможность получить значение атрибута для получения ссылок. 
Перед осуществлением сбора необходимых данных была реализована авторизация на сайте.
Парсинг всех необходимых данных осуществлялся со всех страниц функцией-генератором get_quotes(). Скрипт просматривает каждую страницу и переходит к следующей до тех пор, пока там есть контент.
Все нужные данные находятся по соответствующим им тегам и классам и сохраняются в переменные. Тэги цитат сохраняются в отдельный список, а день рождения каждого автора сохраняется в формате даты и преобразуется в строку. На выходе формируется словарь с данными по цитате. Далее каждый такой словарь добавляется в список all_quotes. Данные выдаются порциями. Перед парсингом новой страницы берётся пауза в 2 секунды. После того, как список all_quotes перестаёт заполняться данными (контент для сбора заканчивается), он записывается в файл quotes.json.

d.	Почему был выбран тот или иной метод/инструмент, а не другой.

В данном случае каждая из страниц прогружается полностью, JavaScript при этом не используется, поэтому, например библиотеку Selenium, использовать в данном случае не нужно.
Использование фреймворка Scrapy также показалось избыточным. Возможно, он был бы более предпочтительным, если бы нужно было собрать больший объём данных, так как он может отработать быстрее и сразу выдать информацию в нужном формате.
Выбор был сделан в пользу BeautifulSoup в связке с requests и lxml. 
За анализ и обработку полученного библиотекой requests  html-кода отвечает lxml. Он может оказаться быстрее и работать корректнее, чем html.parser.
BeautifulSoup позволила легко находить нужные элементы, предварительно реализовав авторизацию на сайте. Использование пауз в работе кода, функции-генератора, а также заголовков позволило сделать скрипт более оптимизированным и похожим на поведение реального пользователя, а не бота. Записать полученные данные в json-файл самостоятельно тоже показалось удобнее, т.к. в итоге получился файл с хорошо читаемыми данными. 
Структуру json-файла хотелось также сделать как можно удобнее для возможного последующего переноса информации в базу данных. Все необходимые сведения можно достать по ключам, а день рождения автора представлен в формате даты.
